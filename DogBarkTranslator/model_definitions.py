# -*- coding: utf-8 -*-
"""model_definitions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h9fK2Klh1OJaHPZnyw7dna6lO5fbMDDV
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
from io import BytesIO
from fastapi import FastAPI, HTTPException
import os



CONTEXT_CLASSES = ["Play", "Aggression", "Neutral"]
NAME_CLASSES = ["Buddy", "Max", "Bella", "Charlie", "Luna", "Rocky", "Lucy", "Daisy", "Milo", "Bailey"]
BREED_CLASSES = ["Labrador", "Poodle", "Beagle", "German Shepherd", "Bulldog", "Golden Retriever"]

# Model definition
class CNN(nn.Module):
    def __init__(self, input_channels=1, num_classes=3):
        super(CNN, self).__init__()

        # Convolutional layers
        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=3, padding="same")
        self.bn1 = nn.BatchNorm2d(16)

        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding="same")
        self.bn2 = nn.BatchNorm2d(32)

        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding="same")
        self.bn3 = nn.BatchNorm2d(64)

        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding="same")
        self.bn4 = nn.BatchNorm2d(128)

        self.dropout = nn.Dropout(p=0.5)

        # Dynamically calculate flattened size
        self.flattened_size = self.get_flattened_size((1, 1, 59, 344))
        self.linear = nn.Linear(self.flattened_size, num_classes)

    def get_flattened_size(self, input_shape):
        with torch.no_grad():
            x = torch.zeros(input_shape)
            x = self.conv1(x)
            x = self.bn1(x)
            x = F.relu(x)
            x = F.max_pool2d(x, 2)

            x = self.conv2(x)
            x = self.bn2(x)
            x = F.relu(x)
            x = F.max_pool2d(x, 2)

            x = self.conv3(x)
            x = self.bn3(x)
            x = F.relu(x)
            x = F.max_pool2d(x, 2)

            x = self.conv4(x)
            x = self.bn4(x)
            x = F.relu(x)
            x = F.max_pool2d(x, 2)

            return x.numel()

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)

        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)

        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)

        x = self.conv4(x)
        x = self.bn4(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)

        x = self.dropout(x)
        x = torch.flatten(x, start_dim=1)
        x = self.linear(x)
        return F.softmax(x, dim=1)

# Function for loading paths into a model
def load_model(model_class, weights_path, num_classes):
    model = CNN(input_channels=1, num_classes=num_classes)
    state_dict = torch.load(weights_path, weights_only=False, map_location=torch.device("cpu"))
    model.load_state_dict(state_dict)
    model.eval()
    return model

# MelSpectrogram transformation setup
SAMPLE_RATE = 16000
N_MELS = 59
HOP_LENGTH = 256
N_FFT = 1024

mel_transform = torchaudio.transforms.MelSpectrogram(
    sample_rate=SAMPLE_RATE,
    n_fft=N_FFT,
    hop_length=HOP_LENGTH,
    n_mels=N_MELS
)

app = FastAPI(max_length=10 * 1024 * 1024)

# Load models
context_model = load_model(CNN, "Models_format_ipynb/weightsForContextPredict.pth", num_classes=3)
name_model = load_model(CNN, "Models_format_ipynb/weightsForNamePredict.pth", num_classes=10)
breed_model = load_model(CNN, "Models_format_ipynb/weightsForBreedPredict.pth", num_classes=6)

@app.get("/")
def root():
    return {"message": "Model API is running"}

@app.post("/predict/")
async def predict(file: UploadFile = File(...)):
    audio_bytes = await file.read()

    
    print(type(audio_bytes))
    print(len(audio_bytes))

    torchaudio.set_audio_backend("sox_io")
    waveform, sample_rate = torchaudio.load(BytesIO(audio_bytes))

    # Resample to match model input
    if sample_rate != SAMPLE_RATE:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=SAMPLE_RATE)
        waveform = resampler(waveform)

    # Mono channel
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # Convert to Mel Spectrogram
    mel_spec = mel_transform(waveform)

    # Pad or crop to width = 344
    if mel_spec.shape[2] < 344:
        pad_amt = 344 - mel_spec.shape[2]
        mel_spec = F.pad(mel_spec, (0, pad_amt))
    else:
        mel_spec = mel_spec[:, :, :344]

    # Add batch dimension: (1, 1, 59, 344)
    mel_spec = mel_spec.unsqueeze(0)

    with torch.no_grad():
        context_pred = context_model(mel_spec)
        name_pred = name_model(mel_spec)
        breed_pred = breed_model(mel_spec)

        context_pred = torch.argmax(context_pred, dim=1).item()
        name_pred = torch.argmax(name_pred, dim=1).item()
        breed_pred = torch.argmax(breed_pred, dim=1).item()

# Return the predictions as a JSON response
    return JSONResponse({
        "context_prediction": CONTEXT_CLASSES[context_pred],
        "name_prediction": NAME_CLASSES[name_pred],
        "breed_prediction": BREED_CLASSES[breed_pred]
    })

@app.get("/predict_sample/{sample_name}")
def predict_sample(sample_name: str):
    file_map = {
        "mac": "audio/Mac-8-P-8b.wav",
        "roodie": "audio/Roodie-8-C-8m.wav",
        "freid": "audio/Freid-A-8a.wav"
    }

    if sample_name not in file_map:
        raise HTTPException(status_code=404, detail="Sample not found.")

    file_path = file_map[sample_name]

    torchaudio.set_audio_backend("sox_io")
    waveform, sample_rate = torchaudio.load(file_path)

    if sample_rate != SAMPLE_RATE:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=SAMPLE_RATE)
        waveform = resampler(waveform)

    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    mel_spec = mel_transform(waveform)

    if mel_spec.shape[2] < 344:
        pad_amt = 344 - mel_spec.shape[2]
        mel_spec = F.pad(mel_spec, (0, pad_amt))
    else:
        mel_spec = mel_spec[:, :, :344]

    mel_spec = mel_spec.unsqueeze(0)

    with torch.no_grad():
        context_pred = context_model(mel_spec)
        name_pred = name_model(mel_spec)
        breed_pred = breed_model(mel_spec)

        context_pred = torch.argmax(context_pred, dim=1).item()
        name_pred = torch.argmax(name_pred, dim=1).item()
        breed_pred = torch.argmax(breed_pred, dim=1).item()

    return [
        {"label": "context_prediction", "confidence": CONTEXT_CLASSES[context_pred]},
        {"label": "name_prediction", "confidence": NAME_CLASSES[name_pred]},
        {"label": "breed_prediction", "confidence": BREED_CLASSES[breed_pred]},
    ]



